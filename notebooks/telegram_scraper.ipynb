{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ee800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded successfully!\n",
      "API_ID: ‚úì\n",
      "API_HASH: ‚úì\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from telethon import TelegramClient\n",
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "from telethon.tl.types import MessageMediaPhoto, MessageMediaDocument\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_ID = os.getenv(\"TELEGRAM_API_ID\")\n",
    "API_HASH = os.getenv(\"TELEGRAM_API_HASH\")\n",
    "\n",
    "print(\"‚úÖ Environment loaded successfully!\")\n",
    "print(f\"API_ID: {'‚úì' if API_ID else '‚úó'}\")\n",
    "print(f\"API_HASH: {'‚úì' if API_HASH else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6368ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 23:11:11,076 - telegram_scraper - INFO - Telegram scraper notebook initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Directory structure created:\n",
      "   Messages: ..\\data\\raw\\telegram_messages\\2025-07-13\n",
      "   Images: ..\\data\\raw\\telegram_images\\2025-07-13\n",
      "üìù Logs: data/logs/\n"
     ]
    }
   ],
   "source": [
    "# Setup comprehensive logging\n",
    "def setup_logging():\n",
    "    \"\"\"Setup comprehensive logging for the scraper\"\"\"\n",
    "    log_dir = Path(\"../data/logs\")\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # File handler for all logs\n",
    "    file_handler = logging.FileHandler(\n",
    "        log_dir / f\"telegram_scraper_{datetime.now().strftime('%Y%m%d')}.log\"\n",
    "    )\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Console handler for important logs\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Setup logger\n",
    "    logger = logging.getLogger('telegram_scraper')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Create directory structure for organized data storage\n",
    "def create_directory_structure(base_date):\n",
    "    \"\"\"Create partitioned directory structure for raw data\"\"\"\n",
    "    base_path = Path(\"../data/raw/telegram_messages\") / base_date\n",
    "    images_path = Path(\"../data/raw/telegram_images\") / base_date\n",
    "    \n",
    "    \n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "    images_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return base_path, images_path\n",
    "\n",
    "# Initialize logging\n",
    "logger = setup_logging()\n",
    "logger.info(\"Telegram scraper notebook initialized\")\n",
    "\n",
    "# Create today's directory structure\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "messages_dir, images_dir = create_directory_structure(today)\n",
    "\n",
    "print(f\"üìÅ Directory structure created:\")\n",
    "print(f\"   Messages: {messages_dir}\")\n",
    "print(f\"   Images: {images_dir}\")\n",
    "print(f\"üìù Logs: data/logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daec388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ethiopian Medical Telegram Channels Discovered:\n",
      "‚úÖ Verified channels: 3\n",
      "   üìã lobelia4cosmetics: Lobelia Pharmacy and Cosmetics (pharmacy_cosmetics)\n",
      "   üìã tikvahpharma: Tikvah Pharma (pharmacy)\n",
      "   üìã CheMed123: CheMed (medical_equipment)\n",
      "\n",
      "üéØ Channels selected for scraping: ['lobelia4cosmetics', 'tikvahpharma', 'CheMed123']\n"
     ]
    }
   ],
   "source": [
    "# Ethiopian Medical Telegram Channels Discovery\n",
    "def discover_ethiopian_medical_channels():\n",
    "    \"\"\"\n",
    "    Discover and organize Ethiopian medical Telegram channels\n",
    "    This includes known channels and channels from et.tgstat.com/medicine\n",
    "    \"\"\"\n",
    "    \n",
    "    # Core verified Ethiopian medical channels\n",
    "    verified_channels = {\n",
    "        \"lobelia4cosmetics\": {\n",
    "            \"name\": \"Lobelia Pharmacy and Cosmetics\",\n",
    "            \"url\": \"https://t.me/lobelia4cosmetics\",\n",
    "            \"category\": \"pharmacy_cosmetics\",\n",
    "            \"verified\": True,\n",
    "            \"priority\": \"high\"\n",
    "        },\n",
    "        \"tikvahpharma\": {\n",
    "            \"name\": \"Tikvah Pharma\",\n",
    "            \"url\": \"https://t.me/tikvahpharma\", \n",
    "            \"category\": \"pharmacy\",\n",
    "            \"verified\": True,\n",
    "            \"priority\": \"high\"\n",
    "        },\n",
    "        \"CheMed123\": {\n",
    "            \"name\": \"CheMed\",\n",
    "            \"url\": \"https://t.me/CheMed123\",\n",
    "            \"category\": \"medical_equipment\",\n",
    "            \"verified\": True,\n",
    "            \"priority\": \"high\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Combine all channels\n",
    "    all_channels = {**verified_channels}\n",
    "    \n",
    "    return verified_channels\n",
    "\n",
    "# Discover channels\n",
    "verified_channels = discover_ethiopian_medical_channels()\n",
    "\n",
    "print(\"üîç Ethiopian Medical Telegram Channels Discovered:\")\n",
    "print(f\"‚úÖ Verified channels: {len(verified_channels)}\")\n",
    "\n",
    "for username, info in verified_channels.items():\n",
    "    print(f\"   üìã {username}: {info['name']} ({info['category']})\")\n",
    "\n",
    "# Use only verified channels for scraping\n",
    "channels_to_scrape = list(verified_channels.keys())\n",
    "print(f\"\\nüéØ Channels selected for scraping: {channels_to_scrape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d5936fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 23:16:44,009 - telegram_scraper - INFO - Telegram client started successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed in successfully as Emnet; remember to not break the ToS or you will risk an account ban!\n",
      "‚úÖ Client started successfully!\n",
      "üë§ Connected as: Emnet\n",
      "üéØ Ready to scrape 3 verified channels\n"
     ]
    }
   ],
   "source": [
    "# Initialize Telegram Client\n",
    "client = TelegramClient(\"anon\", API_ID, API_HASH)\n",
    "\n",
    "# Start the client asynchronously\n",
    "async def start_client():\n",
    "    await client.start()\n",
    "    me = await client.get_me()\n",
    "    logger.info(\"Telegram client started successfully\")\n",
    "    print(\"‚úÖ Client started successfully!\")\n",
    "    print(f\"üë§ Connected as: {me.first_name}\")\n",
    "    return client\n",
    "\n",
    "# Run the async function\n",
    "await start_client()\n",
    "\n",
    "print(f\"üéØ Ready to scrape {len(channels_to_scrape)} verified channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1f6649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 23:18:52,978 - telegram_scraper - INFO - Starting to scrape channel: lobelia4cosmetics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing with lobelia4cosmetics channel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 23:18:53,193 - telegram_scraper - INFO - Channel info: {'username': 'lobelia4cosmetics', 'title': 'Lobelia pharmacy and cosmetics', 'id': 1666492664, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Scraping channel: Lobelia pharmacy and cosmetics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 23:19:23,670 - telegram_scraper - INFO - Successfully scraped 50 messages from lobelia4cosmetics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scraped 50 messages, 50 media files\n",
      "üíæ Saved to: ..\\data\\raw\\telegram_messages\\2025-07-13\\lobelia4cosmetics_2025-07-13.json\n",
      "‚úÖ Test completed: 50 messages, 50 media files\n"
     ]
    }
   ],
   "source": [
    "async def download_media(message, channel_name, images_path):\n",
    "    \"\"\"Download images and media from messages\"\"\"\n",
    "    media_info = []\n",
    "    \n",
    "    try:\n",
    "        if message.media:\n",
    "            if isinstance(message.media, (MessageMediaPhoto, MessageMediaDocument)):\n",
    "                # Create channel-specific directory\n",
    "                channel_images_path = images_path / channel_name\n",
    "                channel_images_path.mkdir(exist_ok=True)\n",
    "                \n",
    "                # Generate filename\n",
    "                timestamp = message.date.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"{channel_name}_{message.id}_{timestamp}\"\n",
    "                \n",
    "                # Download media\n",
    "                try:\n",
    "                    path = await client.download_media(\n",
    "                        message.media, \n",
    "                        file=str(channel_images_path / filename)\n",
    "                    )\n",
    "                    if path:\n",
    "                        media_info.append({\n",
    "                            'type': 'photo' if isinstance(message.media, MessageMediaPhoto) else 'document',\n",
    "                            'filename': os.path.basename(path),\n",
    "                            'path': str(path),\n",
    "                            'size': os.path.getsize(path) if os.path.exists(path) else 0\n",
    "                        })\n",
    "                        logger.debug(f\"Downloaded media: {path}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to download media for message {message.id}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process media for message {message.id}: {e}\")\n",
    "    \n",
    "    return media_info\n",
    "\n",
    "async def scrape_channel_messages(channel_username, limit=1000):\n",
    "    \"\"\"Enhanced channel scraping with image collection and better data structure\"\"\"\n",
    "    logger.info(f\"Starting to scrape channel: {channel_username}\")\n",
    "    \n",
    "    try:\n",
    "        # Get the channel entity\n",
    "        channel = await client.get_entity(channel_username)\n",
    "        channel_info = {\n",
    "            'username': channel_username,\n",
    "            'title': channel.title,\n",
    "            'id': channel.id,\n",
    "            'participants_count': getattr(channel, 'participants_count', None),\n",
    "            'description': getattr(channel, 'about', None)\n",
    "        }\n",
    "        \n",
    "        print(f\"üîÑ Scraping channel: {channel.title}\")\n",
    "        logger.info(f\"Channel info: {channel_info}\")\n",
    "        \n",
    "        # Get messages with enhanced data collection\n",
    "        messages = []\n",
    "        media_count = 0\n",
    "        \n",
    "        async for message in client.iter_messages(channel, limit=limit):\n",
    "            # Collect media if present\n",
    "            media_info = await download_media(message, channel_username, images_dir)\n",
    "            if media_info:\n",
    "                media_count += len(media_info)\n",
    "            \n",
    "            # Enhanced message data structure\n",
    "            message_data = {\n",
    "                'id': message.id,\n",
    "                'date': message.date.isoformat(),\n",
    "                'text': message.text,\n",
    "                'views': message.views,\n",
    "                'forwards': message.forwards,\n",
    "                'replies': message.replies.replies if message.replies else 0,\n",
    "                'reactions': getattr(message, 'reactions', None),\n",
    "                'media': media_info,\n",
    "                'has_media': bool(message.media),\n",
    "                'channel': channel_username,\n",
    "                'channel_info': channel_info,\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "            messages.append(message_data)\n",
    "        \n",
    "        # Save messages to partitioned structure\n",
    "        filename = messages_dir / f\"{channel_username}_{today}.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'channel_info': channel_info,\n",
    "                'scrape_metadata': {\n",
    "                    'scraped_at': datetime.now().isoformat(),\n",
    "                    'total_messages': len(messages),\n",
    "                    'media_files': media_count,\n",
    "                    'scraper_version': '2.0'\n",
    "                },\n",
    "                'messages': messages\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"Successfully scraped {len(messages)} messages from {channel_username}\")\n",
    "        print(f\"‚úÖ Scraped {len(messages)} messages, {media_count} media files\")\n",
    "        print(f\"üíæ Saved to: {filename}\")\n",
    "        \n",
    "        return messages, media_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping {channel_username}: {str(e)}\")\n",
    "        print(f\"‚ùå Error scraping {channel_username}: {str(e)}\")\n",
    "        return [], 0\n",
    "\n",
    "# Test with one channel first\n",
    "print(\"üß™ Testing with lobelia4cosmetics channel...\")\n",
    "test_messages, test_media = await scrape_channel_messages(\"lobelia4cosmetics\", limit=50)\n",
    "print(f\"‚úÖ Test completed: {len(test_messages)} messages, {test_media} media files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
