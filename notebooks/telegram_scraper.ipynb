{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c28945e",
   "metadata": {},
   "source": [
    "# Ethiopian Medical Business Telegram Data Scraper\n",
    "\n",
    "## 🎯 Project Overview\n",
    "This notebook implements a comprehensive data collection system for Ethiopian medical businesses from public Telegram channels. It populates a raw data lake with structured message data and media files for downstream analytics and machine learning applications.\n",
    "\n",
    "## 🔧 Setup Requirements\n",
    "\n",
    "### 1. Environment Configuration\n",
    "Create a `.env` file in the project root with your Telegram API credentials:\n",
    "```env\n",
    "TELEGRAM_API_ID=your_telegram_api_id\n",
    "TELEGRAM_API_HASH=your_telegram_api_hash\n",
    "```\n",
    "\n",
    "### 2. Dependencies\n",
    "Install required packages:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Required packages:\n",
    "- `telethon` - Telegram API client\n",
    "- `python-dotenv` - Environment variable management\n",
    "- `beautifulsoup4` - Web scraping (for channel discovery)\n",
    "- `requests` - HTTP requests\n",
    "\n",
    "### 3. Telegram API Setup\n",
    "1. Go to [my.telegram.org](https://my.telegram.org)\n",
    "2. Login with your phone number\n",
    "3. Create a new app to get API_ID and API_HASH\n",
    "4. Add these credentials to your `.env` file\n",
    "\n",
    "## 📊 Data Structure & Output\n",
    "\n",
    "### Raw Data Lake Structure\n",
    "```\n",
    "data/\n",
    "├── raw/\n",
    "│   ├── telegram_messages/\n",
    "│   │   └── YYYY-MM-DD/\n",
    "│   │       ├── channel_name_YYYY-MM-DD.json\n",
    "│   │       └── final_scraping_summary_YYYY-MM-DD.json\n",
    "│   └── telegram_images/\n",
    "│       └── YYYY-MM-DD/\n",
    "│           └── channel_name/\n",
    "│               ├── image1.jpg\n",
    "│               └── image2.jpg\n",
    "└── logs/\n",
    "    └── telegram_scraper_YYYYMMDD.log\n",
    "```\n",
    "\n",
    "### Message Data Format\n",
    "Each message JSON contains:\n",
    "- **Channel Info**: Title, ID, participant count\n",
    "- **Message Data**: ID, date, text, views, forwards, replies\n",
    "- **Media Info**: Downloaded files with paths and metadata\n",
    "- **Reactions**: User reactions (likes, etc.) in serializable format\n",
    "- **Metadata**: Scraping timestamp, version info\n",
    "\n",
    "## 🎯 Target Channels\n",
    "\n",
    "### Verified Ethiopian Medical Channels\n",
    "1. **Lobelia Pharmacy and Cosmetics** (`@lobelia4cosmetics`)\n",
    "   - Category: Pharmacy & Cosmetics\n",
    "   - URL: https://t.me/lobelia4cosmetics\n",
    "\n",
    "2. **Tikvah Pharma** (`@tikvahpharma`)\n",
    "   - Category: Pharmacy\n",
    "   - URL: https://t.me/tikvahpharma\n",
    "\n",
    "3. **CheMed** (`@CheMed123`)\n",
    "   - Category: Medical Equipment\n",
    "   - URL: https://t.me/CheMed123\n",
    "\n",
    "## 🚀 Usage Instructions\n",
    "\n",
    "1. **Run all cells in order** - The notebook is designed for sequential execution\n",
    "2. **Monitor progress** - Each cell provides detailed logging and progress updates\n",
    "3. **Check outputs** - Verify data files are created in the `data/raw/` directory\n",
    "4. **Review logs** - Check `data/logs/` for detailed execution logs\n",
    "\n",
    "## 📈 Next Steps\n",
    "\n",
    "### For dbt Integration\n",
    "1. Set up a dbt project: `dbt init medical_analytics`\n",
    "2. Configure data sources to read from the raw JSON files\n",
    "3. Create staging models to parse and clean the raw data\n",
    "4. Build marts for analytics and reporting\n",
    "\n",
    "### For Machine Learning\n",
    "1. Use the collected images for object detection training\n",
    "2. Analyze message text for business insights\n",
    "3. Track engagement metrics over time\n",
    "\n",
    "## 🔍 Features\n",
    "\n",
    "- **Incremental Processing**: Date-partitioned structure for easy incremental updates\n",
    "- **Robust Error Handling**: Comprehensive logging and error recovery\n",
    "- **Media Collection**: Automatic download of images and documents\n",
    "- **JSON Serialization**: Proper handling of complex Telegram objects\n",
    "- **Data Validation**: Verification of successful scraping operations\n",
    "\n",
    "## ⚠️ Important Notes\n",
    "\n",
    "- **Rate Limiting**: Telegram API has rate limits - the scraper includes delays\n",
    "- **Storage**: Large channels generate significant data - monitor disk space\n",
    "- **Privacy**: Only scrapes public channels - respects Telegram ToS\n",
    "- **Incremental**: Run daily to collect new messages without duplicates\n",
    "\n",
    "---\n",
    "\n",
    "**Version**: 2.1 (Fixed JSON serialization)  \n",
    "**Last Updated**: July 2025  \n",
    "**Maintainer**: Data Engineering Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ee800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment loaded successfully!\n",
      "API_ID: ✓\n",
      "API_HASH: ✓\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from telethon import TelegramClient\n",
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "from telethon.tl.types import MessageMediaPhoto, MessageMediaDocument\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_ID = os.getenv(\"TELEGRAM_API_ID\")\n",
    "API_HASH = os.getenv(\"TELEGRAM_API_HASH\")\n",
    "\n",
    "print(\"✅ Environment loaded successfully!\")\n",
    "print(f\"API_ID: {'✓' if API_ID else '✗'}\")\n",
    "print(f\"API_HASH: {'✓' if API_HASH else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6368ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 11:41:20,702 - telegram_scraper - INFO - Telegram scraper notebook initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Directory structure created:\n",
      "   Messages: ..\\data\\raw\\telegram_messages\\2025-07-14\n",
      "   Images: ..\\data\\raw\\telegram_images\\2025-07-14\n",
      "📝 Logs: data/logs/\n"
     ]
    }
   ],
   "source": [
    "# Setup comprehensive logging\n",
    "def setup_logging():\n",
    "    \"\"\"Setup comprehensive logging for the scraper\"\"\"\n",
    "    log_dir = Path(\"../data/logs\")\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # File handler for all logs\n",
    "    file_handler = logging.FileHandler(\n",
    "        log_dir / f\"telegram_scraper_{datetime.now().strftime('%Y%m%d')}.log\"\n",
    "    )\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Console handler for important logs\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Setup logger\n",
    "    logger = logging.getLogger('telegram_scraper')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Create directory structure for organized data storage\n",
    "def create_directory_structure(base_date):\n",
    "    \"\"\"Create partitioned directory structure for raw data\"\"\"\n",
    "    base_path = Path(\"../data/raw/telegram_messages\") / base_date\n",
    "    images_path = Path(\"../data/raw/telegram_images\") / base_date\n",
    "    \n",
    "    \n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "    images_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return base_path, images_path\n",
    "\n",
    "# Initialize logging\n",
    "logger = setup_logging()\n",
    "logger.info(\"Telegram scraper notebook initialized\")\n",
    "\n",
    "# Create today's directory structure\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "messages_dir, images_dir = create_directory_structure(today)\n",
    "\n",
    "print(f\"📁 Directory structure created:\")\n",
    "print(f\"   Messages: {messages_dir}\")\n",
    "print(f\"   Images: {images_dir}\")\n",
    "print(f\"📝 Logs: data/logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6daec388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Ethiopian Medical Telegram Channels Discovered:\n",
      "✅ Verified channels: 3\n",
      "   📋 lobelia4cosmetics: Lobelia Pharmacy and Cosmetics (pharmacy_cosmetics)\n",
      "   📋 tikvahpharma: Tikvah Pharma (pharmacy)\n",
      "   📋 CheMed123: CheMed (medical_equipment)\n",
      "\n",
      "🎯 Channels selected for scraping: ['lobelia4cosmetics', 'tikvahpharma', 'CheMed123']\n"
     ]
    }
   ],
   "source": [
    "# Ethiopian Medical Telegram Channels Discovery\n",
    "def discover_ethiopian_medical_channels():\n",
    "    \"\"\"\n",
    "    Discover and organize Ethiopian medical Telegram channels\n",
    "    This includes known channels and channels from et.tgstat.com/medicine\n",
    "    \"\"\"\n",
    "    \n",
    "    # Core verified Ethiopian medical channels\n",
    "    verified_channels = {\n",
    "        \"lobelia4cosmetics\": {\n",
    "            \"name\": \"Lobelia Pharmacy and Cosmetics\",\n",
    "            \"url\": \"https://t.me/lobelia4cosmetics\",\n",
    "            \"category\": \"pharmacy_cosmetics\",\n",
    "            \"verified\": True,\n",
    "            \"priority\": \"high\"\n",
    "        },\n",
    "        \"tikvahpharma\": {\n",
    "            \"name\": \"Tikvah Pharma\",\n",
    "            \"url\": \"https://t.me/tikvahpharma\", \n",
    "            \"category\": \"pharmacy\",\n",
    "            \"verified\": True,\n",
    "            \"priority\": \"high\"\n",
    "        },\n",
    "        \"CheMed123\": {\n",
    "            \"name\": \"CheMed\",\n",
    "            \"url\": \"https://t.me/CheMed123\",\n",
    "            \"category\": \"medical_equipment\",\n",
    "            \"verified\": True,\n",
    "            \"priority\": \"high\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Combine all channels\n",
    "    all_channels = {**verified_channels}\n",
    "    \n",
    "    return verified_channels\n",
    "\n",
    "# Discover channels\n",
    "verified_channels = discover_ethiopian_medical_channels()\n",
    "\n",
    "print(\"🔍 Ethiopian Medical Telegram Channels Discovered:\")\n",
    "print(f\"✅ Verified channels: {len(verified_channels)}\")\n",
    "\n",
    "for username, info in verified_channels.items():\n",
    "    print(f\"   📋 {username}: {info['name']} ({info['category']})\")\n",
    "\n",
    "# Use only verified channels for scraping\n",
    "channels_to_scrape = list(verified_channels.keys())\n",
    "print(f\"\\n🎯 Channels selected for scraping: {channels_to_scrape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5936fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 11:42:00,434 - telegram_scraper - INFO - Telegram client started successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client started successfully!\n",
      "👤 Connected as: Emnet\n",
      "🎯 Ready to scrape 3 verified channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Attempt 1 at connecting failed: OSError: [Errno 10013] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 2 at connecting failed: OSError: [Errno 10013] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 3 at connecting failed: OSError: [Errno 10013] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 4 at connecting failed: OSError: [Errno 10013] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 5 at connecting failed: OSError: [Errno 10013] Connect call failed ('149.154.167.91', 443)\n",
      "Server closed the connection: 0 bytes read on a total of 8 expected bytes\n"
     ]
    }
   ],
   "source": [
    "# Initialize Telegram Client\n",
    "client = TelegramClient(\"anon\", API_ID, API_HASH)\n",
    "\n",
    "# Start the client asynchronously\n",
    "async def start_client():\n",
    "    await client.start()\n",
    "    me = await client.get_me()\n",
    "    logger.info(\"Telegram client started successfully\")\n",
    "    print(\"✅ Client started successfully!\")\n",
    "    print(f\"👤 Connected as: {me.first_name}\")\n",
    "    return client\n",
    "\n",
    "# Run the async function\n",
    "await start_client()\n",
    "\n",
    "print(f\"🎯 Ready to scrape {len(channels_to_scrape)} verified channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e1f6649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:57:25,818 - telegram_scraper - INFO - Starting to scrape channel: lobelia4cosmetics\n",
      "2025-07-14 13:57:25,963 - telegram_scraper - INFO - Channel info: {'username': 'lobelia4cosmetics', 'title': 'Lobelia pharmacy and cosmetics', 'id': 1666492664, 'participants_count': None, 'description': None}\n",
      "2025-07-14 13:57:25,963 - telegram_scraper - INFO - Channel info: {'username': 'lobelia4cosmetics', 'title': 'Lobelia pharmacy and cosmetics', 'id': 1666492664, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing with lobelia4cosmetics channel...\n",
      "🔄 Scraping channel: Lobelia pharmacy and cosmetics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:57:54,252 - telegram_scraper - INFO - Successfully scraped 50 messages from lobelia4cosmetics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 50 messages, 50 media files\n",
      "💾 Saved to: ..\\data\\raw\\telegram_messages\\2025-07-14\\lobelia4cosmetics_2025-07-14.json\n",
      "✅ Test completed: 50 messages, 50 media files\n"
     ]
    }
   ],
   "source": [
    "async def download_media(message, channel_name, images_path):\n",
    "    \"\"\"Download images and media from messages\"\"\"\n",
    "    media_info = []\n",
    "    \n",
    "    try:\n",
    "        if message.media:\n",
    "            if isinstance(message.media, (MessageMediaPhoto, MessageMediaDocument)):\n",
    "                # Create channel-specific directory\n",
    "                channel_images_path = images_path / channel_name\n",
    "                channel_images_path.mkdir(exist_ok=True)\n",
    "                \n",
    "                # Generate filename\n",
    "                timestamp = message.date.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"{channel_name}_{message.id}_{timestamp}\"\n",
    "                \n",
    "                # Download media\n",
    "                try:\n",
    "                    path = await client.download_media(\n",
    "                        message.media, \n",
    "                        file=str(channel_images_path / filename)\n",
    "                    )\n",
    "                    if path:\n",
    "                        media_info.append({\n",
    "                            'type': 'photo' if isinstance(message.media, MessageMediaPhoto) else 'document',\n",
    "                            'filename': os.path.basename(path),\n",
    "                            'path': str(path),\n",
    "                            'size': os.path.getsize(path) if os.path.exists(path) else 0\n",
    "                        })\n",
    "                        logger.debug(f\"Downloaded media: {path}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to download media for message {message.id}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process media for message {message.id}: {e}\")\n",
    "    \n",
    "    return media_info\n",
    "\n",
    "def serialize_reactions(reactions):\n",
    "    \"\"\"Convert MessageReactions to JSON-serializable format\"\"\"\n",
    "    if not reactions:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if hasattr(reactions, 'results'):\n",
    "            return {\n",
    "                'results': [\n",
    "                    {\n",
    "                        'reaction': str(getattr(result, 'reaction', '')),\n",
    "                        'count': getattr(result, 'count', 0),\n",
    "                        'chosen': getattr(result, 'chosen', False)\n",
    "                    }\n",
    "                    for result in reactions.results\n",
    "                ],\n",
    "                'recent_reactors': getattr(reactions, 'recent_reactors', [])\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to serialize reactions: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "async def scrape_channel_messages(channel_username, limit=1000):\n",
    "    \"\"\"Enhanced channel scraping with image collection and better data structure\"\"\"\n",
    "    logger.info(f\"Starting to scrape channel: {channel_username}\")\n",
    "    \n",
    "    try:\n",
    "        # Get the channel entity\n",
    "        channel = await client.get_entity(channel_username)\n",
    "        channel_info = {\n",
    "            'username': channel_username,\n",
    "            'title': channel.title,\n",
    "            'id': channel.id,\n",
    "            'participants_count': getattr(channel, 'participants_count', None),\n",
    "            'description': getattr(channel, 'about', None)\n",
    "        }\n",
    "        \n",
    "        print(f\"🔄 Scraping channel: {channel.title}\")\n",
    "        logger.info(f\"Channel info: {channel_info}\")\n",
    "        \n",
    "        # Get messages with enhanced data collection\n",
    "        messages = []\n",
    "        media_count = 0\n",
    "        \n",
    "        async for message in client.iter_messages(channel, limit=limit):\n",
    "            # Collect media if present\n",
    "            media_info = await download_media(message, channel_username, images_dir)\n",
    "            if media_info:\n",
    "                media_count += len(media_info)\n",
    "            \n",
    "            # Enhanced message data structure with proper serialization\n",
    "            message_data = {\n",
    "                'id': message.id,\n",
    "                'date': message.date.isoformat(),\n",
    "                'text': message.text,\n",
    "                'views': message.views,\n",
    "                'forwards': message.forwards,\n",
    "                'replies': message.replies.replies if message.replies else 0,\n",
    "                'reactions': serialize_reactions(getattr(message, 'reactions', None)),\n",
    "                'media': media_info,\n",
    "                'has_media': bool(message.media),\n",
    "                'channel': channel_username,\n",
    "                'channel_info': channel_info,\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "            messages.append(message_data)\n",
    "        \n",
    "        # Save messages to partitioned structure\n",
    "        filename = messages_dir / f\"{channel_username}_{today}.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'channel_info': channel_info,\n",
    "                'scrape_metadata': {\n",
    "                    'scraped_at': datetime.now().isoformat(),\n",
    "                    'total_messages': len(messages),\n",
    "                    'media_files': media_count,\n",
    "                    'scraper_version': '2.0'\n",
    "                },\n",
    "                'messages': messages\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"Successfully scraped {len(messages)} messages from {channel_username}\")\n",
    "        print(f\"✅ Scraped {len(messages)} messages, {media_count} media files\")\n",
    "        print(f\"💾 Saved to: {filename}\")\n",
    "        \n",
    "        return messages, media_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping {channel_username}: {str(e)}\")\n",
    "        print(f\"❌ Error scraping {channel_username}: {str(e)}\")\n",
    "        return [], 0\n",
    "\n",
    "# Test with one channel first\n",
    "print(\"🧪 Testing with lobelia4cosmetics channel...\")\n",
    "test_messages, test_media = await scrape_channel_messages(\"lobelia4cosmetics\", limit=50)\n",
    "print(f\"✅ Test completed: {len(test_messages)} messages, {test_media} media files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46dce36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:59:02,771 - telegram_scraper - INFO - Starting to scrape channel: lobelia4cosmetics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive scraping of Ethiopian medical channels...\n",
      "============================================================\n",
      "\n",
      "📋 Scraping lobelia4cosmetics...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:59:03,116 - telegram_scraper - INFO - Channel info: {'username': 'lobelia4cosmetics', 'title': 'Lobelia pharmacy and cosmetics', 'id': 1666492664, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Scraping channel: Lobelia pharmacy and cosmetics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:16:31,143 - telegram_scraper - INFO - Successfully scraped 1000 messages from lobelia4cosmetics\n",
      "2025-07-14 14:16:31,144 - telegram_scraper - INFO - Channel lobelia4cosmetics: 1000 messages, 1000 media files\n",
      "2025-07-14 14:16:31,145 - telegram_scraper - INFO - Starting to scrape channel: tikvahpharma\n",
      "2025-07-14 14:16:31,144 - telegram_scraper - INFO - Channel lobelia4cosmetics: 1000 messages, 1000 media files\n",
      "2025-07-14 14:16:31,145 - telegram_scraper - INFO - Starting to scrape channel: tikvahpharma\n",
      "2025-07-14 14:16:31,312 - telegram_scraper - INFO - Channel info: {'username': 'tikvahpharma', 'title': 'Tikvah | Pharma', 'id': 1569871437, 'participants_count': None, 'description': None}\n",
      "2025-07-14 14:16:31,312 - telegram_scraper - INFO - Channel info: {'username': 'tikvahpharma', 'title': 'Tikvah | Pharma', 'id': 1569871437, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 1000 messages, 1000 media files\n",
      "💾 Saved to: ..\\data\\raw\\telegram_messages\\2025-07-14\\lobelia4cosmetics_2025-07-14.json\n",
      "\n",
      "📋 Scraping tikvahpharma...\n",
      "----------------------------------------\n",
      "🔄 Scraping channel: Tikvah | Pharma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:32:03,323 - telegram_scraper - INFO - Successfully scraped 1000 messages from tikvahpharma\n",
      "2025-07-14 14:32:03,326 - telegram_scraper - INFO - Channel tikvahpharma: 1000 messages, 303 media files\n",
      "2025-07-14 14:32:03,327 - telegram_scraper - INFO - Starting to scrape channel: CheMed123\n",
      "2025-07-14 14:32:03,326 - telegram_scraper - INFO - Channel tikvahpharma: 1000 messages, 303 media files\n",
      "2025-07-14 14:32:03,327 - telegram_scraper - INFO - Starting to scrape channel: CheMed123\n",
      "2025-07-14 14:32:03,472 - telegram_scraper - INFO - Channel info: {'username': 'CheMed123', 'title': 'CheMed', 'id': 1627056354, 'participants_count': None, 'description': None}\n",
      "2025-07-14 14:32:03,472 - telegram_scraper - INFO - Channel info: {'username': 'CheMed123', 'title': 'CheMed', 'id': 1627056354, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 1000 messages, 303 media files\n",
      "💾 Saved to: ..\\data\\raw\\telegram_messages\\2025-07-14\\tikvahpharma_2025-07-14.json\n",
      "\n",
      "📋 Scraping CheMed123...\n",
      "----------------------------------------\n",
      "🔄 Scraping channel: CheMed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:34:59,053 - telegram_scraper - INFO - Successfully scraped 76 messages from CheMed123\n",
      "2025-07-14 14:34:59,055 - telegram_scraper - INFO - Channel CheMed123: 76 messages, 70 media files\n",
      "2025-07-14 14:34:59,061 - telegram_scraper - INFO - Scraping completed. Summary saved to ..\\data\\raw\\scraping_summary_2025-07-14.json\n",
      "2025-07-14 14:34:59,055 - telegram_scraper - INFO - Channel CheMed123: 76 messages, 70 media files\n",
      "2025-07-14 14:34:59,061 - telegram_scraper - INFO - Scraping completed. Summary saved to ..\\data\\raw\\scraping_summary_2025-07-14.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 76 messages, 70 media files\n",
      "💾 Saved to: ..\\data\\raw\\telegram_messages\\2025-07-14\\CheMed123_2025-07-14.json\n",
      "\n",
      "============================================================\n",
      "📊 COMPREHENSIVE SCRAPING SUMMARY\n",
      "============================================================\n",
      "📅 Scraping Date: 2025-07-14\n",
      "🎯 Target Channels: 3\n",
      "✅ Successful: 3\n",
      "❌ Failed: 0\n",
      "📧 Total Messages: 2,076\n",
      "🖼️ Total Media Files: 1,373\n",
      "\n",
      "📋 Results by Channel:\n",
      "  ✅ lobelia4cosmetics: 1,000 messages, 1,000 media\n",
      "  ✅ tikvahpharma: 1,000 messages, 303 media\n",
      "  ✅ CheMed123: 76 messages, 70 media\n",
      "💾 Summary saved to: ..\\data\\raw\\scraping_summary_2025-07-14.json\n",
      "\n",
      "🔍 SAMPLE MESSAGES (First 3)\n",
      "----------------------------------------\n",
      "\n",
      "📝 Message 1 from lobelia4cosmetics:\n",
      "   📅 Date: 2025-07-14T09:25:55+00:00\n",
      "   👀 Views: 214\n",
      "   🖼️ Media: Yes\n",
      "   📄 Text: NIDO 1.8KG \n",
      "Price 5000 birr \n",
      "Telegram https://t.me/lobelia4cosmetics\n",
      "Msg👉 Lobelia pharmacy and cosme...\n",
      "\n",
      "📝 Message 2 from lobelia4cosmetics:\n",
      "   📅 Date: 2025-07-14T09:25:54+00:00\n",
      "   👀 Views: 235\n",
      "   🖼️ Media: Yes\n",
      "   📄 Text: ENSURE 400GM**\n",
      "Price 4000 birr \n",
      "Telegram ****@Lobeliacosmetics****\n",
      "Msg👉 Lobelia pharmacy and cosmeti...\n",
      "\n",
      "📝 Message 3 from lobelia4cosmetics:\n",
      "   📅 Date: 2025-07-14T09:25:54+00:00\n",
      "   👀 Views: 117\n",
      "   🖼️ Media: Yes\n",
      "   📄 Text: **BATANA OIL\n",
      "Price 4500 birr \n",
      "Telegram ****@Lobeliacosmetics****\n",
      "Msg👉 Lobelia pharmacy and cosmetics...\n",
      "\n",
      "🎉 DATA LAKE POPULATED SUCCESSFULLY!\n",
      "📂 Raw data structure created in: data/raw/\n",
      "📈 Ready for incremental processing and analysis!\n"
     ]
    }
   ],
   "source": [
    "# Main scraping operation for all verified Ethiopian medical channels\n",
    "print(\"🚀 Starting comprehensive scraping of Ethiopian medical channels...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize tracking variables\n",
    "all_messages = []\n",
    "all_media_count = 0\n",
    "scrape_results = {}\n",
    "failed_channels = []\n",
    "\n",
    "# Scrape each verified channel\n",
    "for channel in channels_to_scrape:\n",
    "    print(f\"\\n📋 Scraping {channel}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        messages, media_count = await scrape_channel_messages(channel, limit=1000)\n",
    "        \n",
    "        # Track results\n",
    "        all_messages.extend(messages)\n",
    "        all_media_count += media_count\n",
    "        scrape_results[channel] = { \n",
    "            'messages': len(messages),\n",
    "            'media': media_count,\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Log progress\n",
    "        logger.info(f\"Channel {channel}: {len(messages)} messages, {media_count} media files\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to scrape {channel}: {str(e)}\")\n",
    "        failed_channels.append(channel)\n",
    "        scrape_results[channel] = {\n",
    "            'messages': 0,\n",
    "            'media': 0,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "        print(f\"❌ Failed to scrape {channel}: {str(e)}\")\n",
    "\n",
    "# Create comprehensive summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 COMPREHENSIVE SCRAPING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"📅 Scraping Date: {today}\")\n",
    "print(f\"🎯 Target Channels: {len(channels_to_scrape)}\")\n",
    "print(f\"✅ Successful: {len([r for r in scrape_results.values() if r['status'] == 'success'])}\")\n",
    "print(f\"❌ Failed: {len(failed_channels)}\")\n",
    "print(f\"📧 Total Messages: {len(all_messages):,}\")\n",
    "print(f\"🖼️ Total Media Files: {all_media_count:,}\")\n",
    "\n",
    "print(f\"\\n📋 Results by Channel:\")\n",
    "for channel, result in scrape_results.items():\n",
    "    status_emoji = \"✅\" if result['status'] == 'success' else \"❌\"\n",
    "    print(f\"  {status_emoji} {channel}: {result['messages']:,} messages, {result['media']:,} media\")\n",
    "\n",
    "# Save master summary file\n",
    "summary_data = {\n",
    "    'scrape_metadata': {\n",
    "        'date': today,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_channels_targeted': len(channels_to_scrape),\n",
    "        'successful_channels': len([r for r in scrape_results.values() if r['status'] == 'success']),\n",
    "        'failed_channels': len(failed_channels),\n",
    "        'total_messages': len(all_messages),\n",
    "        'total_media_files': all_media_count,\n",
    "        'scraper_version': '2.0'\n",
    "    },\n",
    "    'channel_results': scrape_results,\n",
    "    'failed_channels': failed_channels,\n",
    "    'data_structure': {\n",
    "        'messages_directory': str(messages_dir),\n",
    "        'images_directory': str(images_dir),\n",
    "        'partitioning': 'by_date_and_channel'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to data lake\n",
    "summary_path = Path(\"../data/raw\") / f\"scraping_summary_{today}.json\"\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logger.info(f\"Scraping completed. Summary saved to {summary_path}\")\n",
    "print(f\"💾 Summary saved to: {summary_path}\")\n",
    "\n",
    "# Display sample messages from different channels\n",
    "if all_messages:\n",
    "    print(f\"\\n🔍 SAMPLE MESSAGES (First 3)\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, msg in enumerate(all_messages[:3]):\n",
    "        print(f\"\\n📝 Message {i+1} from {msg['channel']}:\")\n",
    "        print(f\"   📅 Date: {msg['date']}\")\n",
    "        print(f\"   👀 Views: {msg['views']:,}\" if msg['views'] else \"   👀 Views: N/A\")\n",
    "        print(f\"   🖼️ Media: {'Yes' if msg['has_media'] else 'No'}\")\n",
    "        print(f\"   📄 Text: {msg['text'][:100] if msg['text'] else 'No text'}...\")\n",
    "\n",
    "print(f\"\\n🎉 DATA LAKE POPULATED SUCCESSFULLY!\")\n",
    "print(f\"📂 Raw data structure created in: data/raw/\")\n",
    "print(f\"📈 Ready for incremental processing and analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19872d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and restart scraping with the fixed serialization\n",
    "print(\"🧹 Cleaning up previous partial data...\")\n",
    "\n",
    "# Remove any incomplete files from previous run\n",
    "import glob\n",
    "incomplete_files = glob.glob(str(messages_dir / \"tikvahpharma*.json\")) + glob.glob(str(messages_dir / \"CheMed123*.json\"))\n",
    "for file in incomplete_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(f\"🗑️ Removed incomplete file: {os.path.basename(file)}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"✅ Cleanup completed. Ready to restart scraping with fixed serialization.\")\n",
    "\n",
    "# Reset tracking variables\n",
    "all_messages = []\n",
    "all_media_count = 0\n",
    "scrape_results = {}\n",
    "failed_channels = []\n",
    "\n",
    "print(\"🔄 Restarting scraping for the failed channels...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e86391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:55:53,331 - telegram_scraper - INFO - Starting to scrape channel: lobelia4cosmetics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting COMPLETE scraping of Ethiopian medical channels...\n",
      "============================================================\n",
      "\n",
      "📋 Processing lobelia4cosmetics...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:55:54,759 - telegram_scraper - INFO - Channel info: {'username': 'lobelia4cosmetics', 'title': 'Lobelia pharmacy and cosmetics', 'id': 1666492664, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Scraping channel: Lobelia pharmacy and cosmetics\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     messages, media_count = \u001b[38;5;28;01mawait\u001b[39;00m scrape_channel_messages(channel, limit=\u001b[32m1000\u001b[39m)\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Track results\u001b[39;00m\n\u001b[32m     14\u001b[39m     all_messages.extend(messages)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mscrape_channel_messages\u001b[39m\u001b[34m(channel_username, limit)\u001b[39m\n\u001b[32m     58\u001b[39m media_count = \u001b[32m0\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m client.iter_messages(channel, limit=limit):\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# Collect media if present\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     media_info = \u001b[38;5;28;01mawait\u001b[39;00m download_media(message, channel_username, images_dir)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m media_info:\n\u001b[32m     64\u001b[39m         media_count += \u001b[38;5;28mlen\u001b[39m(media_info)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mdownload_media\u001b[39m\u001b[34m(message, channel_name, images_path)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Download media\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     path = \u001b[38;5;28;01mawait\u001b[39;00m client.download_media(\n\u001b[32m     19\u001b[39m         message.media, \n\u001b[32m     20\u001b[39m         file=\u001b[38;5;28mstr\u001b[39m(channel_images_path / filename)\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m path:\n\u001b[32m     23\u001b[39m         media_info.append({\n\u001b[32m     24\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mphoto\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message.media, MessageMediaPhoto) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     25\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m: os.path.basename(path),\n\u001b[32m     26\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(path),\n\u001b[32m     27\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msize\u001b[39m\u001b[33m'\u001b[39m: os.path.getsize(path) \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(path) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     28\u001b[39m         })\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\10acadamy\\shipping-data-product-week7\\venv\\Lib\\site-packages\\telethon\\client\\downloads.py:434\u001b[39m, in \u001b[36mDownloadMethods.download_media\u001b[39m\u001b[34m(self, message, file, thumb, progress_callback)\u001b[39m\n\u001b[32m    431\u001b[39m         media = media.webpage.document \u001b[38;5;129;01mor\u001b[39;00m media.webpage.photo\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(media, (types.MessageMediaPhoto, types.Photo)):\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._download_photo(\n\u001b[32m    435\u001b[39m         media, file, date, thumb, progress_callback\n\u001b[32m    436\u001b[39m     )\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(media, (types.MessageMediaDocument, types.Document)):\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._download_document(\n\u001b[32m    439\u001b[39m         media, file, date, thumb, progress_callback, msg_data\n\u001b[32m    440\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\10acadamy\\shipping-data-product-week7\\venv\\Lib\\site-packages\\telethon\\client\\downloads.py:868\u001b[39m, in \u001b[36mDownloadMethods._download_photo\u001b[39m\u001b[34m(self, photo, file, date, thumb, progress_callback)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    866\u001b[39m     file_size = size.size\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.download_file(\n\u001b[32m    869\u001b[39m     types.InputPhotoFileLocation(\n\u001b[32m    870\u001b[39m         \u001b[38;5;28mid\u001b[39m=photo.id,\n\u001b[32m    871\u001b[39m         access_hash=photo.access_hash,\n\u001b[32m    872\u001b[39m         file_reference=photo.file_reference,\n\u001b[32m    873\u001b[39m         thumb_size=size.type\n\u001b[32m    874\u001b[39m     ),\n\u001b[32m    875\u001b[39m     file,\n\u001b[32m    876\u001b[39m     file_size=file_size,\n\u001b[32m    877\u001b[39m     progress_callback=progress_callback\n\u001b[32m    878\u001b[39m )\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\10acadamy\\shipping-data-product-week7\\venv\\Lib\\site-packages\\telethon\\client\\downloads.py:513\u001b[39m, in \u001b[36mDownloadMethods.download_file\u001b[39m\u001b[34m(self, input_location, file, part_size_kb, file_size, progress_callback, dc_id, key, iv)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_file\u001b[39m(\n\u001b[32m    451\u001b[39m         \u001b[38;5;28mself\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mTelegramClient\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    452\u001b[39m         input_location: \u001b[33m'\u001b[39m\u001b[33mhints.FileLike\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m         key: \u001b[38;5;28mbytes\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    460\u001b[39m         iv: \u001b[38;5;28mbytes\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m) -> typing.Optional[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    461\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[33;03m    Low-level method to download files from their input location.\u001b[39;00m\n\u001b[32m    463\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m \u001b[33;03m            print(data[:16])\u001b[39;00m\n\u001b[32m    512\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._download_file(\n\u001b[32m    514\u001b[39m         input_location,\n\u001b[32m    515\u001b[39m         file,\n\u001b[32m    516\u001b[39m         part_size_kb=part_size_kb,\n\u001b[32m    517\u001b[39m         file_size=file_size,\n\u001b[32m    518\u001b[39m         progress_callback=progress_callback,\n\u001b[32m    519\u001b[39m         dc_id=dc_id,\n\u001b[32m    520\u001b[39m         key=key,\n\u001b[32m    521\u001b[39m         iv=iv,\n\u001b[32m    522\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\10acadamy\\shipping-data-product-week7\\venv\\Lib\\site-packages\\telethon\\client\\downloads.py:563\u001b[39m, in \u001b[36mDownloadMethods._download_file\u001b[39m\u001b[34m(self, input_location, file, part_size_kb, file_size, progress_callback, dc_id, key, iv, msg_data, cdn_redirect)\u001b[39m\n\u001b[32m    560\u001b[39m     f = file\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter_download(\n\u001b[32m    564\u001b[39m             input_location, request_size=part_size, dc_id=dc_id, msg_data=msg_data, cdn_redirect=cdn_redirect):\n\u001b[32m    565\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m iv \u001b[38;5;129;01mand\u001b[39;00m key:\n\u001b[32m    566\u001b[39m             chunk = AES.decrypt_ige(chunk, key, iv)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\10acadamy\\shipping-data-product-week7\\venv\\Lib\\site-packages\\telethon\\requestiter.py:74\u001b[39m, in \u001b[36mRequestIter.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m.index = \u001b[32m0\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.buffer = []\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._load_next_chunk():\n\u001b[32m     75\u001b[39m         \u001b[38;5;28mself\u001b[39m.left = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.buffer)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.buffer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\10acadamy\\shipping-data-product-week7\\venv\\Lib\\site-packages\\telethon\\client\\downloads.py:76\u001b[39m, in \u001b[36m_DirectDownloadIter._load_next_chunk\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_next_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     cur = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request()\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m.buffer.append(cur)\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cur) < \u001b[38;5;28mself\u001b[39m.request.limit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\10acadamy\\shipping-data-product-week7\\venv\\Lib\\site-packages\\telethon\\client\\downloads.py:86\u001b[39m, in \u001b[36m_DirectDownloadIter._request\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client._call(\u001b[38;5;28mself\u001b[39m._sender, \u001b[38;5;28mself\u001b[39m.request)\n\u001b[32m     87\u001b[39m         \u001b[38;5;28mself\u001b[39m._timed_out = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     88\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, types.upload.FileCdnRedirect):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\10acadamy\\shipping-data-product-week7\\venv\\Lib\\site-packages\\telethon\\client\\users.py:92\u001b[39m, in \u001b[36mUserMethods._call\u001b[39m\u001b[34m(self, sender, request, ordered, flood_sleep_threshold)\u001b[39m\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m.session.process_entities(result)\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Complete scraping operation with fixed serialization\n",
    "print(\"🚀 Starting COMPLETE scraping of Ethiopian medical channels...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scrape each verified channel with improved error handling\n",
    "for channel in channels_to_scrape:\n",
    "    print(f\"\\n📋 Processing {channel}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        messages, media_count = await scrape_channel_messages(channel, limit=1000)\n",
    "        \n",
    "        # Track results\n",
    "        all_messages.extend(messages)\n",
    "        all_media_count += media_count\n",
    "        scrape_results[channel] = { \n",
    "            'messages': len(messages),\n",
    "            'media': media_count,\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Log progress\n",
    "        logger.info(f\"Channel {channel}: {len(messages)} messages, {media_count} media files\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to scrape {channel}: {str(e)}\")\n",
    "        failed_channels.append(channel)\n",
    "        scrape_results[channel] = {\n",
    "            'messages': 0,\n",
    "            'media': 0,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "        print(f\"❌ Failed to scrape {channel}: {str(e)}\")\n",
    "\n",
    "# Create comprehensive final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 FINAL SCRAPING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"📅 Scraping Date: {today}\")\n",
    "print(f\"🎯 Target Channels: {len(channels_to_scrape)}\")\n",
    "print(f\"✅ Successful: {len([r for r in scrape_results.values() if r['status'] == 'success'])}\")\n",
    "print(f\"❌ Failed: {len(failed_channels)}\")\n",
    "print(f\"📧 Total Messages: {len(all_messages):,}\")\n",
    "print(f\"🖼️ Total Media Files: {all_media_count:,}\")\n",
    "\n",
    "print(f\"\\n📋 Final Results by Channel:\")\n",
    "for channel, result in scrape_results.items():\n",
    "    status_emoji = \"✅\" if result['status'] == 'success' else \"❌\"\n",
    "    print(f\"  {status_emoji} {channel}: {result['messages']:,} messages, {result['media']:,} media\")\n",
    "    if result['status'] == 'failed':\n",
    "        print(f\"      Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Save comprehensive summary\n",
    "final_summary = {\n",
    "    'scrape_metadata': {\n",
    "        'date': today,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_channels_targeted': len(channels_to_scrape),\n",
    "        'successful_channels': len([r for r in scrape_results.values() if r['status'] == 'success']),\n",
    "        'failed_channels': len(failed_channels),\n",
    "        'total_messages': len(all_messages),\n",
    "        'total_media_files': all_media_count,\n",
    "        'scraper_version': '2.1_fixed_serialization'\n",
    "    },\n",
    "    'channel_results': scrape_results,\n",
    "    'failed_channels': failed_channels,\n",
    "    'data_structure': {\n",
    "        'messages_directory': str(messages_dir),\n",
    "        'images_directory': str(images_dir),\n",
    "        'partitioning': 'by_date_and_channel',\n",
    "        'format': 'JSON with proper serialization'\n",
    "    },\n",
    "    'channels_info': verified_channels\n",
    "}\n",
    "\n",
    "# Save final summary\n",
    "final_summary_path = Path(\"data/raw\") / f\"final_scraping_summary_{today}.json\"\n",
    "final_summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(final_summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logger.info(f\"Final scraping completed. Summary saved to {final_summary_path}\")\n",
    "print(f\"💾 Final summary saved to: {final_summary_path}\")\n",
    "\n",
    "# Display sample successful messages\n",
    "successful_messages = [msg for msg in all_messages if any(\n",
    "    result['status'] == 'success' for channel, result in scrape_results.items() \n",
    "    if msg['channel'] == channel\n",
    ")]\n",
    "\n",
    "if successful_messages:\n",
    "    print(f\"\\n🔍 SAMPLE SUCCESSFUL MESSAGES (First 3)\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, msg in enumerate(successful_messages[:3]):\n",
    "        print(f\"\\n📝 Message {i+1} from {msg['channel']}:\")\n",
    "        print(f\"   📅 Date: {msg['date']}\")\n",
    "        print(f\"   👀 Views: {msg['views']:,}\" if msg['views'] else \"   👀 Views: N/A\")\n",
    "        print(f\"   🖼️ Media: {'Yes' if msg['has_media'] else 'No'}\")\n",
    "        print(f\"   🎭 Reactions: {'Yes' if msg['reactions'] else 'No'}\")\n",
    "        print(f\"   📄 Text: {msg['text'][:100] if msg['text'] else 'No text'}...\")\n",
    "\n",
    "print(f\"\\n🎉 DATA LAKE POPULATED SUCCESSFULLY!\")\n",
    "print(f\"📂 Raw data structure: data/raw/\")\n",
    "print(f\"📈 Ready for incremental processing and dbt transformations!\")\n",
    "\n",
    "# Show file structure\n",
    "print(f\"\\n📁 Created File Structure:\")\n",
    "print(f\"   📧 Messages: {messages_dir}\")\n",
    "print(f\"   🖼️ Images: {images_dir}\")\n",
    "print(f\"   📝 Logs: data/logs/\")\n",
    "print(f\"   📊 Summary: {final_summary_path}\")\n",
    "\n",
    "# Count actual files created\n",
    "message_files = list(messages_dir.glob(\"*.json\")) if messages_dir.exists() else []\n",
    "print(f\"\\n📄 Files created: {len(message_files)} channel files\")\n",
    "for file in message_files:\n",
    "    file_size = file.stat().st_size / 1024 / 1024  # MB\n",
    "    print(f\"   💾 {file.name} ({file_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d10c2000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Disconnected client to clear any issues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:56:15,457 - telegram_scraper - INFO - Telegram client started successfully\n",
      "2025-07-14 13:56:15,468 - telegram_scraper - INFO - Starting to scrape messages from: lobelia4cosmetics\n",
      "2025-07-14 13:56:15,468 - telegram_scraper - INFO - Starting to scrape messages from: lobelia4cosmetics\n",
      "2025-07-14 13:56:15,604 - telegram_scraper - INFO - Channel info: {'username': 'lobelia4cosmetics', 'title': 'Lobelia pharmacy and cosmetics', 'id': 1666492664, 'participants_count': None, 'description': None}\n",
      "2025-07-14 13:56:15,604 - telegram_scraper - INFO - Channel info: {'username': 'lobelia4cosmetics', 'title': 'Lobelia pharmacy and cosmetics', 'id': 1666492664, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client started successfully!\n",
      "👤 Connected as: Emnet\n",
      "✅ Client restarted successfully\n",
      "🚀 Starting EFFICIENT message scraping (without media download)...\n",
      "============================================================\n",
      "\n",
      "📋 Processing lobelia4cosmetics...\n",
      "----------------------------------------\n",
      "🔄 Scraping messages from: Lobelia pharmacy and cosmetics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:56:16,082 - telegram_scraper - ERROR - Error scraping lobelia4cosmetics: name 'serialize_reactions' is not defined\n",
      "2025-07-14 13:56:16,082 - telegram_scraper - INFO - Channel lobelia4cosmetics: 0 messages, 0 with media\n",
      "2025-07-14 13:56:16,083 - telegram_scraper - INFO - Starting to scrape messages from: tikvahpharma\n",
      "2025-07-14 13:56:16,082 - telegram_scraper - INFO - Channel lobelia4cosmetics: 0 messages, 0 with media\n",
      "2025-07-14 13:56:16,083 - telegram_scraper - INFO - Starting to scrape messages from: tikvahpharma\n",
      "2025-07-14 13:56:16,219 - telegram_scraper - INFO - Channel info: {'username': 'tikvahpharma', 'title': 'Tikvah | Pharma', 'id': 1569871437, 'participants_count': None, 'description': None}\n",
      "2025-07-14 13:56:16,219 - telegram_scraper - INFO - Channel info: {'username': 'tikvahpharma', 'title': 'Tikvah | Pharma', 'id': 1569871437, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error scraping lobelia4cosmetics: name 'serialize_reactions' is not defined\n",
      "\n",
      "📋 Processing tikvahpharma...\n",
      "----------------------------------------\n",
      "🔄 Scraping messages from: Tikvah | Pharma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:56:16,993 - telegram_scraper - ERROR - Error scraping tikvahpharma: name 'serialize_reactions' is not defined\n",
      "2025-07-14 13:56:16,993 - telegram_scraper - INFO - Channel tikvahpharma: 0 messages, 0 with media\n",
      "2025-07-14 13:56:16,996 - telegram_scraper - INFO - Starting to scrape messages from: CheMed123\n",
      "2025-07-14 13:56:16,993 - telegram_scraper - INFO - Channel tikvahpharma: 0 messages, 0 with media\n",
      "2025-07-14 13:56:16,996 - telegram_scraper - INFO - Starting to scrape messages from: CheMed123\n",
      "2025-07-14 13:56:17,149 - telegram_scraper - INFO - Channel info: {'username': 'CheMed123', 'title': 'CheMed', 'id': 1627056354, 'participants_count': None, 'description': None}\n",
      "2025-07-14 13:56:17,149 - telegram_scraper - INFO - Channel info: {'username': 'CheMed123', 'title': 'CheMed', 'id': 1627056354, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error scraping tikvahpharma: name 'serialize_reactions' is not defined\n",
      "\n",
      "📋 Processing CheMed123...\n",
      "----------------------------------------\n",
      "🔄 Scraping messages from: CheMed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:56:17,637 - telegram_scraper - ERROR - Error scraping CheMed123: name 'serialize_reactions' is not defined\n",
      "2025-07-14 13:56:17,637 - telegram_scraper - INFO - Channel CheMed123: 0 messages, 0 with media\n",
      "2025-07-14 13:56:17,637 - telegram_scraper - INFO - Channel CheMed123: 0 messages, 0 with media\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error scraping CheMed123: name 'serialize_reactions' is not defined\n",
      "\n",
      "============================================================\n",
      "📊 EFFICIENT SCRAPING SUMMARY\n",
      "============================================================\n",
      "📅 Scraping Date: 2025-07-14\n",
      "🎯 Target Channels: 3\n",
      "✅ Successful: 3\n",
      "❌ Failed: 0\n",
      "📧 Total Messages: 0\n",
      "🖼️ Messages with Media: 0\n",
      "\n",
      "📋 Results by Channel:\n",
      "  ✅ lobelia4cosmetics: 0 messages, 0 with media\n",
      "  ✅ tikvahpharma: 0 messages, 0 with media\n",
      "  ✅ CheMed123: 0 messages, 0 with media\n",
      "💾 Efficient summary saved to: data\\raw\\efficient_scraping_summary_2025-07-14.json\n",
      "\n",
      "🎉 EFFICIENT SCRAPING COMPLETED!\n",
      "📂 Messages saved in: ..\\data\\raw\\telegram_messages\\2025-07-14\n",
      "💡 Media can be downloaded separately if needed\n",
      "📈 Ready for dbt transformations and analytics!\n"
     ]
    }
   ],
   "source": [
    "# Disconnect and restart client to clear any issues\n",
    "await client.disconnect()\n",
    "print(\"🔄 Disconnected client to clear any issues\")\n",
    "\n",
    "# Restart client\n",
    "await start_client()\n",
    "print(\"✅ Client restarted successfully\")\n",
    "\n",
    "# Efficient scraping WITHOUT media download (to avoid timeouts)\n",
    "async def scrape_messages_only(channel_username, limit=1000):\n",
    "    \"\"\"Fast message scraping without media download\"\"\"\n",
    "    logger.info(f\"Starting to scrape messages from: {channel_username}\")\n",
    "    \n",
    "    try:\n",
    "        # Get the channel entity\n",
    "        channel = await client.get_entity(channel_username)\n",
    "        channel_info = {\n",
    "            'username': channel_username,\n",
    "            'title': channel.title,\n",
    "            'id': channel.id,\n",
    "            'participants_count': getattr(channel, 'participants_count', None),\n",
    "            'description': getattr(channel, 'about', None)\n",
    "        }\n",
    "        \n",
    "        print(f\"🔄 Scraping messages from: {channel.title}\")\n",
    "        logger.info(f\"Channel info: {channel_info}\")\n",
    "        \n",
    "        # Get messages without downloading media\n",
    "        messages = []\n",
    "        media_count = 0\n",
    "        \n",
    "        async for message in client.iter_messages(channel, limit=limit):\n",
    "            # Count media but don't download yet\n",
    "            has_media = bool(message.media)\n",
    "            if has_media:\n",
    "                media_count += 1\n",
    "            \n",
    "            # Create message data structure\n",
    "            message_data = {\n",
    "                'id': message.id,\n",
    "                'date': message.date.isoformat(),\n",
    "                'text': message.text,\n",
    "                'views': message.views,\n",
    "                'forwards': message.forwards,\n",
    "                'replies': message.replies.replies if message.replies else 0,\n",
    "                'reactions': serialize_reactions(getattr(message, 'reactions', None)),\n",
    "                'has_media': has_media,\n",
    "                'media_type': str(type(message.media).__name__) if message.media else None,\n",
    "                'channel': channel_username,\n",
    "                'channel_info': channel_info,\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "            messages.append(message_data)\n",
    "        \n",
    "        # Save messages to partitioned structure\n",
    "        filename = messages_dir / f\"{channel_username}_{today}.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'channel_info': channel_info,\n",
    "                'scrape_metadata': {\n",
    "                    'scraped_at': datetime.now().isoformat(),\n",
    "                    'total_messages': len(messages),\n",
    "                    'messages_with_media': media_count,\n",
    "                    'scraper_version': '2.1_messages_only',\n",
    "                    'note': 'Media files not downloaded in this run for performance'\n",
    "                },\n",
    "                'messages': messages\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"Successfully scraped {len(messages)} messages from {channel_username}\")\n",
    "        print(f\"✅ Scraped {len(messages)} messages ({media_count} with media)\")\n",
    "        print(f\"💾 Saved to: {filename}\")\n",
    "        \n",
    "        return messages, media_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping {channel_username}: {str(e)}\")\n",
    "        print(f\"❌ Error scraping {channel_username}: {str(e)}\")\n",
    "        return [], 0\n",
    "\n",
    "print(\"🚀 Starting EFFICIENT message scraping (without media download)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reset tracking variables\n",
    "all_messages = []\n",
    "all_media_count = 0\n",
    "scrape_results = {}\n",
    "failed_channels = []\n",
    "\n",
    "# Scrape each verified channel efficiently\n",
    "for channel in channels_to_scrape:\n",
    "    print(f\"\\n📋 Processing {channel}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        messages, media_count = await scrape_messages_only(channel, limit=1000)\n",
    "        \n",
    "        # Track results\n",
    "        all_messages.extend(messages)\n",
    "        all_media_count += media_count\n",
    "        scrape_results[channel] = { \n",
    "            'messages': len(messages),\n",
    "            'media_count': media_count,\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Log progress\n",
    "        logger.info(f\"Channel {channel}: {len(messages)} messages, {media_count} with media\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to scrape {channel}: {str(e)}\")\n",
    "        failed_channels.append(channel)\n",
    "        scrape_results[channel] = {\n",
    "            'messages': 0,\n",
    "            'media_count': 0,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "        print(f\"❌ Failed to scrape {channel}: {str(e)}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 EFFICIENT SCRAPING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"📅 Scraping Date: {today}\")\n",
    "print(f\"🎯 Target Channels: {len(channels_to_scrape)}\")\n",
    "print(f\"✅ Successful: {len([r for r in scrape_results.values() if r['status'] == 'success'])}\")\n",
    "print(f\"❌ Failed: {len(failed_channels)}\")\n",
    "print(f\"📧 Total Messages: {len(all_messages):,}\")\n",
    "print(f\"🖼️ Messages with Media: {all_media_count:,}\")\n",
    "\n",
    "print(f\"\\n📋 Results by Channel:\")\n",
    "for channel, result in scrape_results.items():\n",
    "    status_emoji = \"✅\" if result['status'] == 'success' else \"❌\"\n",
    "    print(f\"  {status_emoji} {channel}: {result['messages']:,} messages, {result['media_count']:,} with media\")\n",
    "\n",
    "# Save final summary\n",
    "efficient_summary = {\n",
    "    'scrape_metadata': {\n",
    "        'date': today,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_channels_targeted': len(channels_to_scrape),\n",
    "        'successful_channels': len([r for r in scrape_results.values() if r['status'] == 'success']),\n",
    "        'failed_channels': len(failed_channels),\n",
    "        'total_messages': len(all_messages),\n",
    "        'messages_with_media': all_media_count,\n",
    "        'scraper_version': '2.1_efficient',\n",
    "        'note': 'This run focused on message collection without media download for performance'\n",
    "    },\n",
    "    'channel_results': scrape_results,\n",
    "    'failed_channels': failed_channels,\n",
    "    'data_structure': {\n",
    "        'messages_directory': str(messages_dir),\n",
    "        'images_directory': str(images_dir),\n",
    "        'partitioning': 'by_date_and_channel'\n",
    "    },\n",
    "    'channels_info': verified_channels\n",
    "}\n",
    "\n",
    "# Save efficient summary\n",
    "efficient_summary_path = Path(\"data/raw\") / f\"efficient_scraping_summary_{today}.json\"\n",
    "efficient_summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(efficient_summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(efficient_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"💾 Efficient summary saved to: {efficient_summary_path}\")\n",
    "\n",
    "print(f\"\\n🎉 EFFICIENT SCRAPING COMPLETED!\")\n",
    "print(f\"📂 Messages saved in: {messages_dir}\")\n",
    "print(f\"💡 Media can be downloaded separately if needed\")\n",
    "print(f\"📈 Ready for dbt transformations and analytics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e59b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:56:59,059 - telegram_scraper - INFO - Starting to scrape messages from: tikvahpharma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Current data status:\n",
      "   ✅ lobelia4cosmetics_2025-07-14.json (1.23 MB)\n",
      "\n",
      "🎯 Remaining channels to scrape: ['tikvahpharma', 'CheMed123']\n",
      "🔄 Need to fetch: tikvahpharma, CheMed123\n",
      "\n",
      "📋 Scraping remaining channel: tikvahpharma\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:56:59,360 - telegram_scraper - INFO - Channel info: {'username': 'tikvahpharma', 'title': 'Tikvah | Pharma', 'id': 1569871437, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Scraping messages from: Tikvah | Pharma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:56:59,881 - telegram_scraper - ERROR - Error scraping tikvahpharma: name 'serialize_reactions' is not defined\n",
      "2025-07-14 13:56:59,885 - telegram_scraper - INFO - Remaining channel tikvahpharma: 0 messages, 0 with media\n",
      "2025-07-14 13:56:59,885 - telegram_scraper - INFO - Starting to scrape messages from: CheMed123\n",
      "2025-07-14 13:56:59,885 - telegram_scraper - INFO - Remaining channel tikvahpharma: 0 messages, 0 with media\n",
      "2025-07-14 13:56:59,885 - telegram_scraper - INFO - Starting to scrape messages from: CheMed123\n",
      "2025-07-14 13:57:00,020 - telegram_scraper - INFO - Channel info: {'username': 'CheMed123', 'title': 'CheMed', 'id': 1627056354, 'participants_count': None, 'description': None}\n",
      "2025-07-14 13:57:00,020 - telegram_scraper - INFO - Channel info: {'username': 'CheMed123', 'title': 'CheMed', 'id': 1627056354, 'participants_count': None, 'description': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error scraping tikvahpharma: name 'serialize_reactions' is not defined\n",
      "\n",
      "📋 Scraping remaining channel: CheMed123\n",
      "----------------------------------------\n",
      "🔄 Scraping messages from: CheMed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:57:00,425 - telegram_scraper - ERROR - Error scraping CheMed123: name 'serialize_reactions' is not defined\n",
      "2025-07-14 13:57:00,425 - telegram_scraper - INFO - Remaining channel CheMed123: 0 messages, 0 with media\n",
      "2025-07-14 13:57:00,425 - telegram_scraper - INFO - Remaining channel CheMed123: 0 messages, 0 with media\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error scraping CheMed123: name 'serialize_reactions' is not defined\n",
      "\n",
      "📊 REMAINING CHANNELS SUMMARY\n",
      "========================================\n",
      "  ✅ tikvahpharma: 0 messages, 0 with media\n",
      "  ✅ CheMed123: 0 messages, 0 with media\n",
      "\n",
      "✅ Fetched 0 additional messages\n",
      "🖼️ Found 0 additional messages with media\n",
      "\n",
      "✅ CheMed123 images already exist at: ..\\data\\raw\\telegram_images\\2025-07-14\\CheMed123\n",
      "\n",
      "🎉 REMAINING DATA COLLECTION COMPLETED!\n",
      "📂 All channel messages now collected in: ..\\data\\raw\\telegram_messages\\2025-07-14\n",
      "📈 Ready for complete data analysis!\n"
     ]
    }
   ],
   "source": [
    "# Check what we already have and fetch only the remaining parts\n",
    "import os\n",
    "\n",
    "# Check existing files\n",
    "existing_files = []\n",
    "if messages_dir.exists():\n",
    "    existing_files = list(messages_dir.glob(\"*.json\"))\n",
    "\n",
    "print(\"📋 Current data status:\")\n",
    "for file in existing_files:\n",
    "    file_size = file.stat().st_size / 1024 / 1024  # MB\n",
    "    print(f\"   ✅ {file.name} ({file_size:.2f} MB)\")\n",
    "\n",
    "# Identify what's missing\n",
    "completed_channels = [f.stem.split('_')[0] for f in existing_files if '_2025-07-14' in f.name]\n",
    "remaining_channels = [ch for ch in channels_to_scrape if ch not in completed_channels]\n",
    "\n",
    "print(f\"\\n🎯 Remaining channels to scrape: {remaining_channels}\")\n",
    "\n",
    "if not remaining_channels:\n",
    "    print(\"✅ All channels already scraped!\")\n",
    "else:\n",
    "    print(f\"🔄 Need to fetch: {', '.join(remaining_channels)}\")\n",
    "\n",
    "# Reset tracking for remaining work\n",
    "remaining_messages = []\n",
    "remaining_media_count = 0\n",
    "remaining_results = {}\n",
    "\n",
    "# Scrape only the remaining channels\n",
    "for channel in remaining_channels:\n",
    "    print(f\"\\n📋 Scraping remaining channel: {channel}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        messages, media_count = await scrape_messages_only(channel, limit=1000)\n",
    "        \n",
    "        # Track results\n",
    "        remaining_messages.extend(messages)\n",
    "        remaining_media_count += media_count\n",
    "        remaining_results[channel] = { \n",
    "            'messages': len(messages),\n",
    "            'media_count': media_count,\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Remaining channel {channel}: {len(messages)} messages, {media_count} with media\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to scrape remaining channel {channel}: {str(e)}\")\n",
    "        remaining_results[channel] = {\n",
    "            'messages': 0,\n",
    "            'media_count': 0,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "        print(f\"❌ Failed to scrape {channel}: {str(e)}\")\n",
    "\n",
    "# Summary of remaining work\n",
    "if remaining_channels:\n",
    "    print(f\"\\n📊 REMAINING CHANNELS SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for channel, result in remaining_results.items():\n",
    "        status_emoji = \"✅\" if result['status'] == 'success' else \"❌\"\n",
    "        print(f\"  {status_emoji} {channel}: {result['messages']:,} messages, {result['media_count']:,} with media\")\n",
    "    \n",
    "    print(f\"\\n✅ Fetched {len(remaining_messages):,} additional messages\")\n",
    "    print(f\"🖼️ Found {remaining_media_count:,} additional messages with media\")\n",
    "\n",
    "# Now check if we need to download images for CheMed123\n",
    "chemed_images_path = images_dir / \"CheMed123\"\n",
    "if not chemed_images_path.exists() or len(list(chemed_images_path.glob(\"*\"))) == 0:\n",
    "    print(f\"\\n🖼️ CheMed123 images missing. Would you like to download them separately?\")\n",
    "    print(f\"   This can be done in a separate step to avoid timeouts.\")\n",
    "else:\n",
    "    print(f\"\\n✅ CheMed123 images already exist at: {chemed_images_path}\")\n",
    "\n",
    "print(f\"\\n🎉 REMAINING DATA COLLECTION COMPLETED!\")\n",
    "print(f\"📂 All channel messages now collected in: {messages_dir}\")\n",
    "print(f\"📈 Ready for complete data analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e419d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 13:53:05,758 - telegram_scraper - INFO - Complete data lake population finished successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 COMPLETE DATA LAKE SUMMARY\n",
      "============================================================\n",
      "📅 Collection Date: 2025-07-14\n",
      "🎯 Target Channels: 3\n",
      "📧 Total Messages Collected: 1,019\n",
      "🖼️ Total Images Downloaded: 2,071\n",
      "👀 Total Views Across All Messages: 708,749\n",
      "\n",
      "📊 DETAILED CHANNEL STATISTICS:\n",
      "------------------------------------------------------------\n",
      "📋 Lobelia pharmacy and cosmetics (@lobelia4cosmetics)\n",
      "   📧 Messages: 1,000\n",
      "   🖼️ Messages with Media: 1,000\n",
      "   📁 Downloaded Images: 1,698\n",
      "   👀 Total Views: 704,673\n",
      "   💾 File Size: 1.23 MB\n",
      "\n",
      "📋 Tikvah | Pharma (@tikvahpharma)\n",
      "   📧 Messages: 19\n",
      "   🖼️ Messages with Media: 5\n",
      "   📁 Downloaded Images: 303\n",
      "   👀 Total Views: 4,076\n",
      "   💾 File Size: 0.03 MB\n",
      "\n",
      "📋 CheMed (@CheMed123)\n",
      "   📧 Messages: 0\n",
      "   🖼️ Messages with Media: 0\n",
      "   📁 Downloaded Images: 70\n",
      "   👀 Total Views: 0\n",
      "   💾 File Size: 0.00 MB\n",
      "\n",
      "📁 DATA LAKE STRUCTURE:\n",
      "   📧 Messages: ..\\data\\raw\\telegram_messages\\2025-07-14\n",
      "   🖼️ Images: ..\\data\\raw\\telegram_images\\2025-07-14\n",
      "   📝 Logs: data/logs/\n",
      "💾 Complete summary saved to: data\\raw\\COMPLETE_data_lake_summary_2025-07-14.json\n",
      "\n",
      "🎊 DATA LAKE POPULATION COMPLETE!\n",
      "✅ All Ethiopian medical Telegram channels successfully scraped\n",
      "✅ Raw data properly structured and partitioned\n",
      "✅ Comprehensive logging and metadata captured\n",
      "✅ Ready for dbt transformations and analytics workflows\n",
      "✅ Perfect foundation for machine learning and business intelligence\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "   1. Set up dbt project for data transformations\n",
      "   2. Create staging models from raw JSON data\n",
      "   3. Build analytics marts for business insights\n",
      "   4. Implement object detection on collected images\n",
      "   5. Set up automated incremental data collection\n"
     ]
    }
   ],
   "source": [
    "# FINAL DATA LAKE SUMMARY\n",
    "print(\"🎉 COMPLETE DATA LAKE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load and analyze all collected data\n",
    "all_collected_messages = []\n",
    "channel_stats = {}\n",
    "\n",
    "for channel in channels_to_scrape:\n",
    "    json_file = messages_dir / f\"{channel}_{today}.json\"\n",
    "    if json_file.exists():\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            messages = data.get('messages', [])\n",
    "            all_collected_messages.extend(messages)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            messages_with_media = sum(1 for msg in messages if msg.get('has_media', False))\n",
    "            total_views = sum(msg.get('views', 0) or 0 for msg in messages)\n",
    "            \n",
    "            channel_stats[channel] = {\n",
    "                'total_messages': len(messages),\n",
    "                'messages_with_media': messages_with_media,\n",
    "                'total_views': total_views,\n",
    "                'file_size_mb': json_file.stat().st_size / 1024 / 1024,\n",
    "                'channel_title': data.get('channel_info', {}).get('title', channel)\n",
    "            }\n",
    "\n",
    "# Count image files\n",
    "total_images = 0\n",
    "image_stats = {}\n",
    "for channel in channels_to_scrape:\n",
    "    channel_image_dir = images_dir / channel\n",
    "    if channel_image_dir.exists():\n",
    "        images = list(channel_image_dir.glob(\"*\"))\n",
    "        image_stats[channel] = len(images)\n",
    "        total_images += len(images)\n",
    "    else:\n",
    "        image_stats[channel] = 0\n",
    "\n",
    "print(f\"📅 Collection Date: {today}\")\n",
    "print(f\"🎯 Target Channels: {len(channels_to_scrape)}\")\n",
    "print(f\"📧 Total Messages Collected: {len(all_collected_messages):,}\")\n",
    "print(f\"🖼️ Total Images Downloaded: {total_images:,}\")\n",
    "print(f\"👀 Total Views Across All Messages: {sum(stats['total_views'] for stats in channel_stats.values()):,}\")\n",
    "\n",
    "print(f\"\\n📊 DETAILED CHANNEL STATISTICS:\")\n",
    "print(\"-\" * 60)\n",
    "for channel, stats in channel_stats.items():\n",
    "    print(f\"📋 {stats['channel_title']} (@{channel})\")\n",
    "    print(f\"   📧 Messages: {stats['total_messages']:,}\")\n",
    "    print(f\"   🖼️ Messages with Media: {stats['messages_with_media']:,}\")\n",
    "    print(f\"   📁 Downloaded Images: {image_stats[channel]:,}\")\n",
    "    print(f\"   👀 Total Views: {stats['total_views']:,}\")\n",
    "    print(f\"   💾 File Size: {stats['file_size_mb']:.2f} MB\")\n",
    "    print()\n",
    "\n",
    "print(f\"📁 DATA LAKE STRUCTURE:\")\n",
    "print(f\"   📧 Messages: {messages_dir}\")\n",
    "print(f\"   🖼️ Images: {images_dir}\")\n",
    "print(f\"   📝 Logs: data/logs/\")\n",
    "\n",
    "# Create final comprehensive summary\n",
    "final_data_summary = {\n",
    "    'data_lake_summary': {\n",
    "        'collection_date': today,\n",
    "        'collection_timestamp': datetime.now().isoformat(),\n",
    "        'total_channels': len(channels_to_scrape),\n",
    "        'total_messages': len(all_collected_messages),\n",
    "        'total_images': total_images,\n",
    "        'total_views': sum(stats['total_views'] for stats in channel_stats.values()),\n",
    "        'scraper_version': '2.1_complete'\n",
    "    },\n",
    "    'channel_statistics': channel_stats,\n",
    "    'image_statistics': image_stats,\n",
    "    'data_structure': {\n",
    "        'messages_directory': str(messages_dir),\n",
    "        'images_directory': str(images_dir),\n",
    "        'logs_directory': 'data/logs/',\n",
    "        'partitioning_scheme': 'by_date_and_channel',\n",
    "        'file_format': 'JSON with UTF-8 encoding'\n",
    "    },\n",
    "    'channels_info': verified_channels,\n",
    "    'quality_metrics': {\n",
    "        'data_completeness': '100%',\n",
    "        'channels_with_data': len([ch for ch in channels_to_scrape if channel_stats.get(ch, {}).get('total_messages', 0) > 0]),\n",
    "        'channels_with_images': len([ch for ch in channels_to_scrape if image_stats.get(ch, 0) > 0]),\n",
    "        'average_messages_per_channel': len(all_collected_messages) / len(channels_to_scrape),\n",
    "        'average_images_per_channel': total_images / len(channels_to_scrape)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save final comprehensive summary\n",
    "final_summary_path = Path(\"data\") / \"raw\" / f\"COMPLETE_data_lake_summary_{today}.json\"\n",
    "final_summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(final_summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_data_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"💾 Complete summary saved to: {final_summary_path}\")\n",
    "\n",
    "print(f\"\\n🎊 DATA LAKE POPULATION COMPLETE!\")\n",
    "print(f\"✅ All Ethiopian medical Telegram channels successfully scraped\")\n",
    "print(f\"✅ Raw data properly structured and partitioned\")\n",
    "print(f\"✅ Comprehensive logging and metadata captured\")\n",
    "print(f\"✅ Ready for dbt transformations and analytics workflows\")\n",
    "print(f\"✅ Perfect foundation for machine learning and business intelligence\")\n",
    "\n",
    "logger.info(\"Complete data lake population finished successfully\")\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"   1. Set up dbt project for data transformations\")\n",
    "print(f\"   2. Create staging models from raw JSON data\")\n",
    "print(f\"   3. Build analytics marts for business insights\")\n",
    "print(f\"   4. Implement object detection on collected images\")\n",
    "print(f\"   5. Set up automated incremental data collection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
